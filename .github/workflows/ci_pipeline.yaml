name: Backend Architecture CI

# Trigger the tests on push to main, or whenever a PR is opened against main
on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  security-scans:
    name: SAST & Dependency Scanning
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          cache: 'pip'

      - name: Install Security Tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit pip-audit

      - name: Run pip-audit (Dependency CVEs)
        # Scans the root requirements file
        run: pip-audit -r requirements.txt

      - name: Run Bandit (SAST)
        # -ll: Report only medium and high severity issues
        # -ii: Report only medium and high confidence issues
        # -r: Recursive scan of core application folders
        run: bandit -r api/ etl/ src/ -ll -ii
        
  test-and-coverage:
    runs-on: ubuntu-latest
    needs: security-scans
    
    # 1. Spin up the PostgreSQL Service Container
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: local_dev_password
          POSTGRES_DB: pipeline_db
        # Set health checks to wait until postgres has started
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      # 2. Check out the repository code
      - name: Checkout Repository
        uses: actions/checkout@v4

      # 3. Initialize the Database Schema & Roles
      # GitHub Actions services don't auto-mount local folders like docker-compose does, 
      # so we pipe our SQL scripts into the running container directly.
      - name: Initialize Database
        env:
          PGPASSWORD: local_dev_password
        run: |
          # Wait a few seconds to ensure DB is fully accepting connections
          sleep 3 
          psql -h localhost -U postgres -d pipeline_db -f db-init/01_schema.sql
          psql -h localhost -U postgres -d pipeline_db -f db-init/02_security.sql
          psql -h localhost -U postgres -d pipeline_db -f db-init/03_triggers.sql

      # 4. Set up Python
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
          cache: 'pip' # Automatically caches requirements to speed up future runs

      # 5. Install Dependencies
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov httpx

      # 6. Apply Database Migrations (NEW STEP)
      # This upgrades the flat db-init schema to our new biological hierarchy
      - name: Run Alembic Migrations
        run: |
          alembic upgrade head

      # 7. Run the Test Suite (Updated numbering)
      - name: Execute Pytest
        env:
          # Inject the highest-privileged role credentials so conftest.py can set up/tear down
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: pipeline_db
          DB_USER: etl_worker
          DB_PASSWORD: strong_etl_password
          # Inject a dummy encryption key for the security/ETL tests
          ENCRYPTION_KEY: "dummy_base64_encryption_key_for_testing=" 
        run: |
          pytest tests/ \
            --cov=api \
            --cov=etl \
            --cov=src/ont-clinical-pipeline/bin \
            --cov-report=xml \
            --cov-report=term-missing